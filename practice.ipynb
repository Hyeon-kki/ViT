{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "## input image batch 1000개의 Sample RGB 3개의 채널 height 244 X  width 244 크기이다. \n",
    "x = torch.randn(1000, 3, 224, 224)\n",
    "\n",
    "## Image를 이미지의 크기/패치의 크기의 패치로 쪼개고 Flatten 시킨다. \n",
    "p = 16 # 논문에서 16이 계산 비용대비 효율적이라 소개한다. \n",
    "\n",
    "## 이때, p_h(p_w): 패치의 크기 and n_h(n_w): 패치의 크기 나눴을 때 가로 수 \n",
    "patches = rearrange(x, 'b c (p_h n_h) (p_w n_w) -> b (n_h n_w) (p_h p_w c)', p_h=p, p_w=p)\n",
    "\n",
    "\n",
    "########### 결과\n",
    "# x : torch.Size([1000, 3, 224, 224])\n",
    "# patches : torch.Size([1000, 196, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "in_channel = 3\n",
    "patch_size = 16\n",
    "embedding = pow(patch_size, 2) * in_channel\n",
    "\n",
    "patch_layer = nn.Sequential(\n",
    "    nn.Conv2d(in_channel, embedding, kernel_size= patch_size, stride= patch_size),\n",
    "    Rearrange('b e h w -> b (h w) e')\n",
    ").cuda()\n",
    "##  input을 여러 개 받는 다면 summary(model, [(1, 16, 16), (1, 28, 28)]) 이렇게 리스트로 담아주자\n",
    "summary(patch_layer,  x.shape[1:], device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Class token and Position Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (여담이지만, 개인적으로 class token 없어도 학습이 잘 이루어지는걸 확인했는데, 모델의 마지막 layer인 linear layer에서 모델의 모든 feature들이 계산되는걸 보면 굳이 없어도 되지 않은가 싶었다.)\n",
    "# patch 당 Class token이 붙는다. (왜 사진 당 클래스 토큰을 안붙였지? 내가 아는 클래스가 아닌 것인가...??)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7092,  0.5827,  0.5065]],\n",
       "\n",
       "        [[-0.2832,  0.3169, -2.6648]],\n",
       "\n",
       "        [[-1.4175, -0.6300, -2.4862]],\n",
       "\n",
       "        [[-1.6847, -0.3376, -1.1439]],\n",
       "\n",
       "        [[-2.4180,  0.4842, -2.0808]],\n",
       "\n",
       "        [[-1.4570, -1.1090, -2.6160]],\n",
       "\n",
       "        [[-1.2922, -1.6965, -0.8728]],\n",
       "\n",
       "        [[ 0.6710, -0.1460, -1.6055]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((8,1,3))\n",
    "y = torch.randn((1,3))\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
