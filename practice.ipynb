{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "## input image batch 1000개의 Sample RGB 3개의 채널 height 244 X  width 244 크기이다. \n",
    "x = torch.randn(1000, 3, 224, 224)\n",
    "\n",
    "## Image를 이미지의 크기/패치의 크기의 패치로 쪼개고 Flatten 시킨다. \n",
    "p = 16 # 논문에서 16이 계산 비용대비 효율적이라 소개한다. \n",
    "\n",
    "## 이때, p_h(p_w): 패치의 크기 and n_h(n_w): 패치의 크기 나눴을 때 가로 수 \n",
    "patches = rearrange(x, 'b c (p_h n_h) (p_w n_w) -> b (n_h n_w) (p_h p_w c)', p_h=p, p_w=p)\n",
    "\n",
    "\n",
    "########### 결과\n",
    "# x : torch.Size([1000, 3, 224, 224])\n",
    "# patches : torch.Size([1000, 196, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "in_channel = 3\n",
    "patch_size = 16\n",
    "embedding = pow(patch_size, 2) * in_channel\n",
    "\n",
    "patch_layer = nn.Sequential(\n",
    "    nn.Conv2d(in_channel, embedding, kernel_size= patch_size, stride= patch_size),\n",
    "    Rearrange('b e h w -> b (h w) e')\n",
    ").cuda()\n",
    "##  input을 여러 개 받는 다면 summary(model, [(1, 16, 16), (1, 28, 28)]) 이렇게 리스트로 담아주자\n",
    "summary(patch_layer,  x.shape[1:], device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Class token and Position Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (여담이지만, 개인적으로 class token 없어도 학습이 잘 이루어지는걸 확인했는데, 모델의 마지막 layer인 linear layer에서 모델의 모든 feature들이 계산되는걸 보면 굳이 없어도 되지 않은가 싶었다.)\n",
    "# patch 당 Class token이 붙는다. (왜 사진 당 클래스 토큰을 안붙였지? 내가 아는 클래스가 아닌 것인가...??)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0645,  0.0319, -0.0629,  ...,  0.0329, -0.0006,  0.0061],\n",
      "        [ 0.1477, -0.0162,  0.0389,  ...,  0.0314, -0.0117,  0.0088],\n",
      "        [-0.0173, -0.0504, -0.0197,  ...,  0.0237, -0.0635, -0.1069],\n",
      "        ...,\n",
      "        [-0.0596, -0.0419, -0.0436,  ..., -0.0769, -0.0583, -0.0541],\n",
      "        [ 0.0225,  0.0889, -0.0031,  ...,  0.0488, -0.0606,  0.0227],\n",
      "        [ 0.0221,  0.1643,  0.0616,  ...,  0.0945, -0.1091,  0.0279]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (386627056.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    *[1,2,3]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "*[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
